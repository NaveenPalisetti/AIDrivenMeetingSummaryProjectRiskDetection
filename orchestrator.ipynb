{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab Setup for Streamlit Client\n",
    "\n",
    "**Important:** This notebook only runs the Streamlit client.\n",
    "\n",
    "\n",
    "- You must run your FastAPI backend (uvicorn) on your own machine or a public server.\n",
    "- Start ngrok on the backend machine and copy the HTTPS URL.\n",
    "- Update the API_URL in orchestrator_streamlit_client.py to match your ngrok URL.\n",
    "- Do NOT try to run the FastAPI backend in Colab.\n",
    "- If you get a 405 error, double-check the backend is running and the URL is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Streamlit App with GPU in Google Colab\n",
    "\n",
    "This notebook will:\n",
    "- Clone your project from GitHub\n",
    "- Install all requirements\n",
    "- Enable GPU support\n",
    "- Launch Streamlit using Colab's GPU\n",
    "- Expose the Streamlit UI via ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check GPU availability\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU is available:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU detected. Go to Runtime > Change runtime type > select GPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%cd /content\n",
    "!rm -rf AIDrivenSummarizationProjectManagement\n",
    "# 2. Clone your project repository\n",
    "!git clone https://github.com/NaveenPalisetti/AIDrivenMeetingSummaryProjectRiskDetection.git\n",
    "%cd AIDrivenMeetingSummaryProjectRiskDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Install requirements (including GPU-enabled torch if needed)\n",
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118 --quiet\n",
    "!pip install -r requirements.txt --quiet\n",
    "!pip install streamlit pyngrok --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Set up ngrok (replace with your own authtoken if needed)\n",
    "!ngrok config add-authtoken 37SZRBmhzpCfMAms5WnqAUVpttn_5xWd1fbo38pEPMYw1YCn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Kill old processes\n",
    "!pkill streamlit\n",
    "!pkill ngrok\n",
    "from pyngrok import ngrok\n",
    "ngrok.kill()\n",
    "!fuser -k 8000/tcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Launch Streamlit app with ngrok tunnel\n",
    "import os\n",
    "from pyngrok import ngrok\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Kill previous tunnels if any\n",
    "ngrok.kill()\n",
    "\n",
    "# Start Streamlit in a background thread\n",
    "streamlit_thread = threading.Thread(target=lambda: os.system('streamlit run orchestrator_streamlit_client.py --server.port 8501 --server.headless true'))\n",
    "streamlit_thread.start()\n",
    "\n",
    "# Wait longer to ensure Streamlit is up before starting ngrok\n",
    "print('Waiting 20 seconds for Streamlit to start...')\n",
    "time.sleep(20)\n",
    "\n",
    "# Start ngrok tunnel for Streamlit (port 8501)\n",
    "public_url = ngrok.connect(8501, bind_tls=True)\n",
    "print('Streamlit public URL:', public_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Copy Mistral model from Google Drive to local disk for faster loading\n",
    "import shutil, os\n",
    "src = '/content/drive/MyDrive/Dissertation/Project/models/mistral-7B-Instruct-v0.2'\n",
    "dst = '/content/mistral-7B-Instruct-v0.2'\n",
    "if not os.path.exists(dst):\n",
    "    print('Copying Mistral model from Drive to local disk...')\n",
    "    shutil.copytree(src, dst)\n",
    "else:\n",
    "    print('Local copy of Mistral model already exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4b. Start FastAPI backend in background with port check\n",
    "!pip install uvicorn --quiet\n",
    "!uvicorn mcp.server.mcp_api:app --host 0.0.0.0 --port 8000 --reload &\n",
    "import time, socket\n",
    "print('Waiting 10 seconds for FastAPI backend to start...')\n",
    "time.sleep(10)\n",
    "sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "result = sock.connect_ex(('localhost', 8000))\n",
    "print('Port 8000 status:', 'open' if result == 0 else 'closed')\n",
    "sock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1768564167700,
     "user": {
      "displayName": "Naveen Palisetti",
      "userId": "06513417714719156805"
     },
     "user_tz": -330
    },
    "id": "U3b0axFHgGEw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOc5YXhRY7iGvdc0H/O9vsT",
   "mount_file_id": "16PHJ-5zUFbcH5L_e569Ye7m384XQijJ9",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
